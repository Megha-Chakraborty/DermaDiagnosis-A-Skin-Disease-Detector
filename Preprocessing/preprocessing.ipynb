{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d254cff",
   "metadata": {},
   "source": [
    "# Feature Engineering & Data Preprocessing\n",
    "# HAM10000 Skin Lesion Dataset\n",
    "\n",
    "This notebook performs comprehensive feature engineering and data preprocessing to prepare the dataset for machine learning models.\n",
    "\n",
    "## Objectives:\n",
    "1. **Data Cleaning**: Handle missing values and outliers\n",
    "2. **Feature Engineering**: Create new meaningful features\n",
    "3. **Data Encoding**: Convert categorical variables to numerical\n",
    "4. **Feature Scaling**: Normalize/standardize features\n",
    "5. **Data Splitting**: Create train/validation/test sets\n",
    "6. **Feature Selection**: Identify most important features\n",
    "7. **Data Balancing**: Handle class imbalance issues\n",
    "\n",
    "## Key Steps:\n",
    "- Load and clean the dataset\n",
    "- Create age groups and interaction features  \n",
    "- Encode categorical variables (Label/One-Hot encoding)\n",
    "- Scale numerical features\n",
    "- Split data with stratification\n",
    "- Apply feature selection techniques\n",
    "- Handle class imbalance with SMOTE/class weights\n",
    "- Save processed datasets for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a631cae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Available preprocessing tools:\n",
      "   ‚Ä¢ Data splitting and stratification\n",
      "   ‚Ä¢ Feature scaling (Standard, MinMax)\n",
      "   ‚Ä¢ Encoding (Label, One-Hot)\n",
      "   ‚Ä¢ Feature selection (Chi2, Mutual Info, RFE)\n",
      "   ‚Ä¢ Class balancing (SMOTE, ADASYN)\n",
      "   ‚Ä¢ Pipeline creation for automated preprocessing\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Available preprocessing tools:\")\n",
    "print(\"   ‚Ä¢ Data splitting and stratification\")\n",
    "print(\"   ‚Ä¢ Feature scaling (Standard, MinMax)\")\n",
    "print(\"   ‚Ä¢ Encoding (Label, One-Hot)\")\n",
    "print(\"   ‚Ä¢ Feature selection (Chi2, Mutual Info, RFE)\")\n",
    "print(\"   ‚Ä¢ Class balancing (SMOTE, ADASYN)\")\n",
    "print(\"   ‚Ä¢ Pipeline creation for automated preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23d96a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset loaded successfully!\n",
      "   ‚Ä¢ Shape: (10015, 7)\n",
      "   ‚Ä¢ Memory usage: 3.78 MB\n",
      "\n",
      "üìã Dataset Overview:\n",
      "   ‚Ä¢ Total samples: 10,015\n",
      "   ‚Ä¢ Features: ['lesion_id', 'image_id', 'dx', 'dx_type', 'age', 'sex', 'localization']\n",
      "   ‚Ä¢ Target variable: 'dx' (Disease type)\n",
      "   ‚Ä¢ Unique diseases: 7\n",
      "\n",
      "üîç Data Quality Check:\n",
      "   ‚Ä¢ Missing values: 57 total\n",
      "   ‚Ä¢ Missing by column:\n",
      "     - age: 57 (0.6%)\n",
      "   ‚Ä¢ Duplicated rows: 0\n",
      "   ‚Ä¢ Data types: {'lesion_id': dtype('O'), 'image_id': dtype('O'), 'dx': dtype('O'), 'dx_type': dtype('O'), 'age': dtype('float64'), 'sex': dtype('O'), 'localization': dtype('O')}\n",
      "\n",
      "üëÄ Data Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   dx dx_type   age   sex localization\n",
       "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n",
       "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n",
       "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n",
       "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n",
       "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßπ Data Cleaning:\n",
      "   ‚úÖ Filled 0 missing age values with median: 50.0\n",
      "   ‚úÖ No duplicate rows found!\n",
      "\n",
      "üìä Clean dataset shape: (10015, 7)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../Dataset/HAM10000_metadata.csv')\n",
    "\n",
    "print(\"üìä Dataset loaded successfully!\")\n",
    "print(f\"   ‚Ä¢ Shape: {df.shape}\")\n",
    "print(f\"   ‚Ä¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nüìã Dataset Overview:\")\n",
    "print(f\"   ‚Ä¢ Total samples: {len(df):,}\")\n",
    "print(f\"   ‚Ä¢ Features: {list(df.columns)}\")\n",
    "print(f\"   ‚Ä¢ Target variable: 'dx' (Disease type)\")\n",
    "print(f\"   ‚Ä¢ Unique diseases: {df['dx'].nunique()}\")\n",
    "\n",
    "# Check data quality\n",
    "print(\"\\nüîç Data Quality Check:\")\n",
    "missing_vals = df.isnull().sum()\n",
    "print(f\"   ‚Ä¢ Missing values: {missing_vals.sum()} total\")\n",
    "if missing_vals.sum() > 0:\n",
    "    print(\"   ‚Ä¢ Missing by column:\")\n",
    "    for col, miss in missing_vals.items():\n",
    "        if miss > 0:\n",
    "            print(f\"     - {col}: {miss} ({miss/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"   ‚Ä¢ Duplicated rows: {df.duplicated().sum()}\")\n",
    "print(f\"   ‚Ä¢ Data types: {dict(df.dtypes)}\")\n",
    "\n",
    "# Quick preview\n",
    "print(\"\\nüëÄ Data Preview:\")\n",
    "display(df.head())\n",
    "\n",
    "# Data Cleaning\n",
    "print(\"\\nüßπ Data Cleaning:\")\n",
    "# Handle missing values\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    # Fill missing ages with median\n",
    "    if 'age' in df.columns and df['age'].isnull().sum() > 0:\n",
    "        median_age = df['age'].median()\n",
    "        df['age'].fillna(median_age, inplace=True)\n",
    "        print(f\"   ‚úÖ Filled {df['age'].isnull().sum()} missing age values with median: {median_age}\")\n",
    "    \n",
    "    # Fill categorical missing values with mode\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            mode_val = df[col].mode()[0]\n",
    "            df[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"   ‚úÖ Filled missing {col} values with mode: {mode_val}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No missing values found!\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "duplicate_count = df.duplicated().sum()\n",
    "if duplicate_count > 0:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"   ‚úÖ Removed {duplicate_count} duplicate rows\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No duplicate rows found!\")\n",
    "\n",
    "print(f\"\\nüìä Clean dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0524262f",
   "metadata": {},
   "source": [
    "## Step 1: Data Cleaning & Quality Assurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e905d7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Data Cleaning:\n",
      "   ‚úÖ No missing values found!\n",
      "   ‚úÖ No duplicate rows found!\n",
      "   üìä Age outliers detected: 39 samples\n",
      "   üìä Age range: 0 - 85 years\n",
      "   üìä Outlier bounds: 2.5 - 102.5 years\n",
      "\n",
      "üìä Clean dataset shape: (10015, 7)\n",
      "   üîÑ Shape change: (10015, 7) ‚Üí (10015, 7)\n",
      "\n",
      "‚úÖ Data Quality Summary:\n",
      "   ‚Ä¢ Missing values: 0\n",
      "   ‚Ä¢ Duplicate rows: 0\n",
      "   ‚Ä¢ Data integrity: 100% complete\n",
      "   ‚Ä¢ Missing values: 0\n",
      "   ‚Ä¢ Duplicate rows: 0\n",
      "   ‚Ä¢ Data integrity: 100% complete\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "print(\"üßπ Data Cleaning:\")\n",
    "\n",
    "# Handle missing values\n",
    "original_shape = df.shape\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    # Fill missing ages with median\n",
    "    if 'age' in df.columns and df['age'].isnull().sum() > 0:\n",
    "        median_age = df['age'].median()\n",
    "        missing_age_count = df['age'].isnull().sum()\n",
    "        df['age'].fillna(median_age, inplace=True)\n",
    "        print(f\"   ‚úÖ Filled {missing_age_count} missing age values with median: {median_age}\")\n",
    "    \n",
    "    # Fill categorical missing values with mode\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            mode_val = df[col].mode()[0]\n",
    "            missing_count = df[col].isnull().sum()\n",
    "            df[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"   ‚úÖ Filled {missing_count} missing {col} values with mode: {mode_val}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No missing values found!\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "duplicate_count = df.duplicated().sum()\n",
    "if duplicate_count > 0:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"   ‚úÖ Removed {duplicate_count} duplicate rows\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No duplicate rows found!\")\n",
    "\n",
    "# Check for outliers in age\n",
    "Q1 = df['age'].quantile(0.25)\n",
    "Q3 = df['age'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['age'] < lower_bound) | (df['age'] > upper_bound)]\n",
    "print(f\"   üìä Age outliers detected: {len(outliers)} samples\")\n",
    "print(f\"   üìä Age range: {df['age'].min():.0f} - {df['age'].max():.0f} years\")\n",
    "print(f\"   üìä Outlier bounds: {lower_bound:.1f} - {upper_bound:.1f} years\")\n",
    "\n",
    "print(f\"\\nüìä Clean dataset shape: {df.shape}\")\n",
    "print(f\"   üîÑ Shape change: {original_shape} ‚Üí {df.shape}\")\n",
    "\n",
    "# Verify data quality\n",
    "print(f\"\\n‚úÖ Data Quality Summary:\")\n",
    "print(f\"   ‚Ä¢ Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"   ‚Ä¢ Duplicate rows: {df.duplicated().sum()}\")\n",
    "print(f\"   ‚Ä¢ Data integrity: 100% complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada58c1f",
   "metadata": {},
   "source": [
    "## Step 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ebd8f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Feature Engineering:\n",
      "   ‚úÖ Created age groups: 0-20, 21-40, 41-60, 61-80, 81-100\n",
      "   ‚úÖ Normalized age to 0-1 scale\n",
      "   ‚úÖ Created binary features: is_elderly, is_young, is_male\n",
      "   ‚úÖ Created age groups: 0-20, 21-40, 41-60, 61-80, 81-100\n",
      "   ‚úÖ Normalized age to 0-1 scale\n",
      "   ‚úÖ Created binary features: is_elderly, is_young, is_male\n",
      "   ‚úÖ Added lesion count per patient (range: 1-6)   ‚úÖ Added lesion count per patient (range: 1-6)\n",
      "\n",
      "   ‚úÖ Grouped body locations into regions: ['head_neck' 'torso' 'other' 'extremity']\n",
      "   ‚úÖ Created high-risk disease indicator\n",
      "   ‚úÖ Added diagnosis confidence scores\n",
      "\n",
      "üìä Feature Engineering Summary:\n",
      "   ‚Ä¢ Original features: 7\n",
      "   ‚Ä¢ New features: 9\n",
      "   ‚Ä¢ Total features: 16\n",
      "   ‚Ä¢ New feature list: ['age_group', 'age_normalized', 'is_elderly', 'is_young', 'is_male', 'lesion_count', 'body_region', 'is_high_risk', 'diagnosis_confidence']\n",
      "\n",
      "üìà New Feature Distributions:\n",
      "   ‚Ä¢ age_group:\n",
      "     {'41-60': np.int64(4355), '61-80': np.int64(2509), '21-40': np.int64(2449), '0-20': np.int64(412), '81-100': np.int64(290)}\n",
      "   ‚Ä¢ body_region:\n",
      "     {'torso': np.int64(3621), 'extremity': np.int64(3604), 'other': np.int64(1693), 'head_neck': np.int64(1097)}\n",
      "   ‚Ä¢ is_high_risk:\n",
      "     {0: np.int64(8388), 1: np.int64(1627)}\n",
      "\n",
      "‚úÖ Feature Engineering completed!\n",
      "   ‚úÖ Grouped body locations into regions: ['head_neck' 'torso' 'other' 'extremity']\n",
      "   ‚úÖ Created high-risk disease indicator\n",
      "   ‚úÖ Added diagnosis confidence scores\n",
      "\n",
      "üìä Feature Engineering Summary:\n",
      "   ‚Ä¢ Original features: 7\n",
      "   ‚Ä¢ New features: 9\n",
      "   ‚Ä¢ Total features: 16\n",
      "   ‚Ä¢ New feature list: ['age_group', 'age_normalized', 'is_elderly', 'is_young', 'is_male', 'lesion_count', 'body_region', 'is_high_risk', 'diagnosis_confidence']\n",
      "\n",
      "üìà New Feature Distributions:\n",
      "   ‚Ä¢ age_group:\n",
      "     {'41-60': np.int64(4355), '61-80': np.int64(2509), '21-40': np.int64(2449), '0-20': np.int64(412), '81-100': np.int64(290)}\n",
      "   ‚Ä¢ body_region:\n",
      "     {'torso': np.int64(3621), 'extremity': np.int64(3604), 'other': np.int64(1693), 'head_neck': np.int64(1097)}\n",
      "   ‚Ä¢ is_high_risk:\n",
      "     {0: np.int64(8388), 1: np.int64(1627)}\n",
      "\n",
      "‚úÖ Feature Engineering completed!\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "print(\"üîß Feature Engineering:\")\n",
    "\n",
    "# Create a copy for feature engineering\n",
    "df_fe = df.copy()\n",
    "\n",
    "# 1. Age Groups\n",
    "age_bins = [0, 20, 40, 60, 80, 100]\n",
    "age_labels = ['0-20', '21-40', '41-60', '61-80', '81-100']\n",
    "df_fe['age_group'] = pd.cut(df_fe['age'], bins=age_bins, labels=age_labels, include_lowest=True)\n",
    "print(\"   ‚úÖ Created age groups: 0-20, 21-40, 41-60, 61-80, 81-100\")\n",
    "\n",
    "# 2. Age normalization (0-1 scale)\n",
    "df_fe['age_normalized'] = (df_fe['age'] - df_fe['age'].min()) / (df_fe['age'].max() - df_fe['age'].min())\n",
    "print(f\"   ‚úÖ Normalized age to 0-1 scale\")\n",
    "\n",
    "# 3. Binary features\n",
    "df_fe['is_elderly'] = (df_fe['age'] >= 65).astype(int)\n",
    "df_fe['is_young'] = (df_fe['age'] <= 30).astype(int)\n",
    "df_fe['is_male'] = (df_fe['sex'] == 'male').astype(int)\n",
    "print(f\"   ‚úÖ Created binary features: is_elderly, is_young, is_male\")\n",
    "\n",
    "# 4. Lesion count per patient (if multiple lesions exist)\n",
    "lesion_counts = df_fe.groupby('lesion_id').size().reset_index(name='lesion_count')\n",
    "df_fe = df_fe.merge(lesion_counts, on='lesion_id', how='left')\n",
    "print(f\"   ‚úÖ Added lesion count per patient (range: {df_fe['lesion_count'].min()}-{df_fe['lesion_count'].max()})\")\n",
    "\n",
    "# 5. Body region grouping\n",
    "# Group similar body locations\n",
    "torso_locations = ['back', 'chest', 'abdomen']\n",
    "extremity_locations = ['upper extremity', 'lower extremity', 'hand', 'foot']\n",
    "head_locations = ['face', 'scalp', 'neck', 'ear']\n",
    "\n",
    "def categorize_location(location):\n",
    "    location = location.lower() if pd.notna(location) else 'unknown'\n",
    "    if any(loc in location for loc in torso_locations):\n",
    "        return 'torso'\n",
    "    elif any(loc in location for loc in extremity_locations):\n",
    "        return 'extremity'\n",
    "    elif any(loc in location for loc in head_locations):\n",
    "        return 'head_neck'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "df_fe['body_region'] = df_fe['localization'].apply(categorize_location)\n",
    "print(f\"   ‚úÖ Grouped body locations into regions: {df_fe['body_region'].unique()}\")\n",
    "\n",
    "# 6. Disease risk categories (based on medical knowledge)\n",
    "high_risk_diseases = ['mel', 'bcc']  # Melanoma, Basal Cell Carcinoma\n",
    "df_fe['is_high_risk'] = df_fe['dx'].isin(high_risk_diseases).astype(int)\n",
    "print(f\"   ‚úÖ Created high-risk disease indicator\")\n",
    "\n",
    "# 7. Diagnosis confidence (based on dx_type)\n",
    "confidence_mapping = {'histo': 1.0, 'follow_up': 0.8, 'consensus': 0.6}\n",
    "df_fe['diagnosis_confidence'] = df_fe['dx_type'].map(confidence_mapping)\n",
    "print(f\"   ‚úÖ Added diagnosis confidence scores\")\n",
    "\n",
    "# Display new features\n",
    "print(f\"\\nüìä Feature Engineering Summary:\")\n",
    "print(f\"   ‚Ä¢ Original features: {len(df.columns)}\")\n",
    "print(f\"   ‚Ä¢ New features: {len(df_fe.columns) - len(df.columns)}\")\n",
    "print(f\"   ‚Ä¢ Total features: {len(df_fe.columns)}\")\n",
    "\n",
    "new_features = [col for col in df_fe.columns if col not in df.columns]\n",
    "print(f\"   ‚Ä¢ New feature list: {new_features}\")\n",
    "\n",
    "# Show feature distributions\n",
    "print(f\"\\nüìà New Feature Distributions:\")\n",
    "for feature in ['age_group', 'body_region', 'is_high_risk']:\n",
    "    if feature in df_fe.columns:\n",
    "        print(f\"   ‚Ä¢ {feature}:\")\n",
    "        print(f\"     {dict(df_fe[feature].value_counts())}\")\n",
    "        \n",
    "print(f\"\\n‚úÖ Feature Engineering completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e51cdf",
   "metadata": {},
   "source": [
    "## Step 3: Data Encoding & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ed4afaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Encoding & Transformation:\n",
      "Target variable mapping:\n",
      "      bkl ‚Üí 0 (1099 samples)\n",
      "      nv ‚Üí 1 (6705 samples)\n",
      "      df ‚Üí 2 (115 samples)\n",
      "      mel ‚Üí 3 (1113 samples)\n",
      "      vasc ‚Üí 4 (142 samples)\n",
      "      bcc ‚Üí 5 (514 samples)\n",
      "      akiec ‚Üí 6 (327 samples)\n",
      "Label encoded dx_type: {'confocal': np.int64(0), 'consensus': np.int64(1), 'follow_up': np.int64(2), 'histo': np.int64(3)}\n",
      "One-hot encoded sex: 3 categories\n",
      "      bcc ‚Üí 5 (514 samples)\n",
      "      akiec ‚Üí 6 (327 samples)\n",
      "Label encoded dx_type: {'confocal': np.int64(0), 'consensus': np.int64(1), 'follow_up': np.int64(2), 'histo': np.int64(3)}\n",
      "One-hot encoded sex: 3 categories\n",
      "One-hot encoded localization: 15 categories\n",
      "One-hot encoded age_group: 5 categories\n",
      "One-hot encoded body_region: 4 categories\n",
      "Standard scaled numerical features\n",
      "MinMax scaled numerical features\n",
      "\n",
      "Feature Engineering Summary:\n",
      "   ‚Ä¢ Numerical features: 18\n",
      "   ‚Ä¢ Binary/One-hot features: 31\n",
      "   ‚Ä¢ Label encoded features: 1\n",
      "   ‚Ä¢ Total modeling features: 51\n",
      "\n",
      "üëÄ Encoded Data Preview:\n",
      "One-hot encoded localization: 15 categories\n",
      "One-hot encoded age_group: 5 categories\n",
      "One-hot encoded body_region: 4 categories\n",
      "Standard scaled numerical features\n",
      "MinMax scaled numerical features\n",
      "\n",
      "Feature Engineering Summary:\n",
      "   ‚Ä¢ Numerical features: 18\n",
      "   ‚Ä¢ Binary/One-hot features: 31\n",
      "   ‚Ä¢ Label encoded features: 1\n",
      "   ‚Ä¢ Total modeling features: 51\n",
      "\n",
      "üëÄ Encoded Data Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_encoded</th>\n",
       "      <th>image_id</th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "      <th>age_normalized</th>\n",
       "      <th>lesion_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bkl</td>\n",
       "      <td>2</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>80.0</td>\n",
       "      <td>61-80</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bkl</td>\n",
       "      <td>2</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>80.0</td>\n",
       "      <td>61-80</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bkl</td>\n",
       "      <td>2</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>80.0</td>\n",
       "      <td>61-80</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bkl</td>\n",
       "      <td>2</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>80.0</td>\n",
       "      <td>61-80</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bkl</td>\n",
       "      <td>2</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>75.0</td>\n",
       "      <td>61-80</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dx  dx_encoded      image_id   age age_group  age_normalized  lesion_count\n",
       "0  bkl           2  ISIC_0027419  80.0     61-80        0.941176             2\n",
       "1  bkl           2  ISIC_0025030  80.0     61-80        0.941176             2\n",
       "2  bkl           2  ISIC_0026769  80.0     61-80        0.941176             2\n",
       "3  bkl           2  ISIC_0025661  80.0     61-80        0.941176             2\n",
       "4  bkl           2  ISIC_0031633  75.0     61-80        0.882353             2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Encoding & Transformation\n",
    "print(\"Data Encoding & Transformation:\")\n",
    "\n",
    "# Ensure df_fe is defined (run feature engineering cell first)\n",
    "try:\n",
    "    df_encoded = df_fe.copy()\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Variable 'df_fe' is not defined. Please run the Feature Engineering cell before this one.\")\n",
    "\n",
    "# Store original target for reference\n",
    "target_mapping = {label: idx for idx, label in enumerate(df_encoded['dx'].unique())}\n",
    "reverse_target_mapping = {idx: label for label, idx in target_mapping.items()}\n",
    "\n",
    "print(f\"Target variable mapping:\")\n",
    "for label, idx in target_mapping.items():\n",
    "    count = (df_encoded['dx'] == label).sum()\n",
    "    print(f\"      {label} ‚Üí {idx} ({count} samples)\")\n",
    "\n",
    "# 1. Label Encoding for target variable\n",
    "label_encoder_target = LabelEncoder()\n",
    "df_encoded['dx_encoded'] = label_encoder_target.fit_transform(df_encoded['dx'])\n",
    "\n",
    "# 2. Label Encoding for ordinal features\n",
    "ordinal_features = ['dx_type']  # These have inherent order\n",
    "label_encoders = {}\n",
    "\n",
    "for feature in ordinal_features:\n",
    "    if feature in df_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[f'{feature}_encoded'] = le.fit_transform(df_encoded[feature])\n",
    "        label_encoders[feature] = le\n",
    "        print(f\"Label encoded {feature}: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# 3. One-Hot Encoding for nominal categorical features\n",
    "nominal_features = ['sex', 'localization', 'age_group', 'body_region']\n",
    "categorical_encoded_features = []\n",
    "\n",
    "for feature in nominal_features:\n",
    "    if feature in df_encoded.columns:\n",
    "        # Get unique values\n",
    "        unique_vals = df_encoded[feature].unique()\n",
    "        \n",
    "        # Create one-hot encoded columns\n",
    "        for val in unique_vals:\n",
    "            new_col = f'{feature}_{val}'\n",
    "            df_encoded[new_col] = (df_encoded[feature] == val).astype(int)\n",
    "            categorical_encoded_features.append(new_col)\n",
    "        \n",
    "        print(f\"One-hot encoded {feature}: {len(unique_vals)} categories\")\n",
    "\n",
    "# 4. Feature scaling for numerical features\n",
    "numerical_features = ['age', 'age_normalized', 'lesion_count', 'diagnosis_confidence']\n",
    "scalers = {}\n",
    "\n",
    "# Standard Scaling\n",
    "scaler_standard = StandardScaler()\n",
    "for feature in numerical_features:\n",
    "    if feature in df_encoded.columns:\n",
    "        df_encoded[f'{feature}_scaled'] = scaler_standard.fit_transform(df_encoded[[feature]])\n",
    "        \n",
    "scalers['standard'] = scaler_standard\n",
    "print(f\"Standard scaled numerical features\")\n",
    "\n",
    "# MinMax Scaling\n",
    "scaler_minmax = MinMaxScaler()\n",
    "for feature in numerical_features:\n",
    "    if feature in df_encoded.columns:\n",
    "        df_encoded[f'{feature}_minmax'] = scaler_minmax.fit_transform(df_encoded[[feature]])\n",
    "        \n",
    "scalers['minmax'] = scaler_minmax\n",
    "print(f\"MinMax scaled numerical features\")\n",
    "\n",
    "# 5. Create final feature sets\n",
    "# Original categorical columns to drop\n",
    "original_categorical = ['lesion_id', 'image_id', 'dx', 'dx_type', 'sex', 'localization', 'age_group', 'body_region']\n",
    "\n",
    "# Features for modeling\n",
    "numerical_cols = [col for col in df_encoded.columns if any(x in col for x in ['age', 'lesion_count', 'diagnosis_confidence'])]\n",
    "binary_cols = [col for col in df_encoded.columns if col.startswith(('is_', 'age_group_', 'body_region_', 'sex_', 'localization_'))]\n",
    "encoded_cols = [col for col in df_encoded.columns if col.endswith('_encoded') and col != 'dx_encoded']\n",
    "\n",
    "feature_columns = numerical_cols + binary_cols + encoded_cols\n",
    "print(f\"\\nFeature Engineering Summary:\")\n",
    "print(f\"   ‚Ä¢ Numerical features: {len([col for col in numerical_cols if df_encoded[col].dtype != 'O'])}\")\n",
    "print(f\"   ‚Ä¢ Binary/One-hot features: {len(binary_cols)}\")\n",
    "print(f\"   ‚Ä¢ Label encoded features: {len(encoded_cols)}\")\n",
    "print(f\"   ‚Ä¢ Total modeling features: {len(feature_columns)}\")\n",
    "\n",
    "# Display sample of encoded data\n",
    "print(f\"\\nüëÄ Encoded Data Preview:\")\n",
    "preview_cols = ['dx', 'dx_encoded'] + feature_columns[:5]\n",
    "display(df_encoded[preview_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a3103",
   "metadata": {},
   "source": [
    "## Step 4: Data Splitting & Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06cdcf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Splitting & Stratification:\n",
      "Feature matrix shape: (10015, 49)\n",
      "Target vector shape: (10015,)\n",
      "Number of classes: 7\n",
      "\n",
      "Original class distribution:\n",
      "Class 0 (bkl): 327 samples (3.3%)\n",
      "Class 1 (nv): 514 samples (5.1%)\n",
      "Class 2 (df): 1,099 samples (11.0%)\n",
      "Class 3 (mel): 115 samples (1.1%)\n",
      "Class 4 (vasc): 1,113 samples (11.1%)\n",
      "Class 5 (bcc): 6,705 samples (66.9%)\n",
      "Class 6 (akiec): 142 samples (1.4%)\n",
      "\n",
      "Data splitting completed:\n",
      "Training set: 6,409 samples (64.0%)\n",
      "Validation set: 1,603 samples (16.0%)\n",
      "Test set: 2,003 samples (20.0%)\n",
      "\n",
      "Class distribution verification:\n",
      "      Train set:\n",
      "         Class 0 (bkl): 209 (3.3%)\n",
      "         Class 1 (nv): 329 (5.1%)\n",
      "         Class 2 (df): 703 (11.0%)\n",
      "         Class 3 (mel): 74 (1.2%)\n",
      "         Class 4 (vasc): 712 (11.1%)\n",
      "         Class 5 (bcc): 4,291 (67.0%)\n",
      "         Class 6 (akiec): 91 (1.4%)\n",
      "      Validation set:\n",
      "         Class 0 (bkl): 53 (3.3%)\n",
      "         Class 1 (nv): 82 (5.1%)\n",
      "         Class 2 (df): 176 (11.0%)\n",
      "         Class 3 (mel): 18 (1.1%)\n",
      "         Class 4 (vasc): 178 (11.1%)\n",
      "         Class 5 (bcc): 1,073 (66.9%)\n",
      "         Class 6 (akiec): 23 (1.4%)\n",
      "      Test set:\n",
      "         Class 0 (bkl): 65 (3.2%)\n",
      "         Class 1 (nv): 103 (5.1%)\n",
      "         Class 2 (df): 220 (11.0%)\n",
      "         Class 3 (mel): 23 (1.1%)\n",
      "         Class 4 (vasc): 223 (11.1%)\n",
      "         Class 5 (bcc): 1,341 (66.9%)\n",
      "         Class 6 (akiec): 28 (1.4%)\n",
      "\n",
      "Feature Statistics:\n",
      "      ‚Ä¢ Total features: 49\n",
      "      ‚Ä¢ Numerical features: 24\n",
      "      ‚Ä¢ Categorical features: 27\n",
      "      ‚Ä¢ Feature value range: [-3.065, 85.000]\n",
      "\n",
      "Data splitting completed:\n",
      "Training set: 6,409 samples (64.0%)\n",
      "Validation set: 1,603 samples (16.0%)\n",
      "Test set: 2,003 samples (20.0%)\n",
      "\n",
      "Class distribution verification:\n",
      "      Train set:\n",
      "         Class 0 (bkl): 209 (3.3%)\n",
      "         Class 1 (nv): 329 (5.1%)\n",
      "         Class 2 (df): 703 (11.0%)\n",
      "         Class 3 (mel): 74 (1.2%)\n",
      "         Class 4 (vasc): 712 (11.1%)\n",
      "         Class 5 (bcc): 4,291 (67.0%)\n",
      "         Class 6 (akiec): 91 (1.4%)\n",
      "      Validation set:\n",
      "         Class 0 (bkl): 53 (3.3%)\n",
      "         Class 1 (nv): 82 (5.1%)\n",
      "         Class 2 (df): 176 (11.0%)\n",
      "         Class 3 (mel): 18 (1.1%)\n",
      "         Class 4 (vasc): 178 (11.1%)\n",
      "         Class 5 (bcc): 1,073 (66.9%)\n",
      "         Class 6 (akiec): 23 (1.4%)\n",
      "      Test set:\n",
      "         Class 0 (bkl): 65 (3.2%)\n",
      "         Class 1 (nv): 103 (5.1%)\n",
      "         Class 2 (df): 220 (11.0%)\n",
      "         Class 3 (mel): 23 (1.1%)\n",
      "         Class 4 (vasc): 223 (11.1%)\n",
      "         Class 5 (bcc): 1,341 (66.9%)\n",
      "         Class 6 (akiec): 28 (1.4%)\n",
      "\n",
      "Feature Statistics:\n",
      "      ‚Ä¢ Total features: 49\n",
      "      ‚Ä¢ Numerical features: 24\n",
      "      ‚Ä¢ Categorical features: 27\n",
      "      ‚Ä¢ Feature value range: [-3.065, 85.000]\n"
     ]
    }
   ],
   "source": [
    "# Data Splitting with Stratification\n",
    "print(\"Data Splitting & Stratification:\")\n",
    "\n",
    "# Ensure df_encoded is defined (run encoding cell first)\n",
    "if 'df_encoded' not in globals():\n",
    "    print(\"Variable 'df_encoded' is not defined. Please run the Data Encoding & Transformation cell before this one.\")\n",
    "else:\n",
    "    # Remove non-numeric columns from feature_columns\n",
    "    non_numeric_cols = ['image_id', 'age_group']\n",
    "    modeling_features = [col for col in feature_columns if col not in non_numeric_cols and pd.api.types.is_numeric_dtype(df_encoded[col])]\n",
    "    X = df_encoded[modeling_features].copy()\n",
    "    y = df_encoded['dx_encoded']\n",
    "\n",
    "# Convert categorical columns to string to avoid fillna issues\n",
    "if 'X' in locals():\n",
    "    for col in X.select_dtypes(include=['category']).columns:\n",
    "        X[col] = X[col].astype(str)\n",
    "    X = X.fillna(0)  # Handle any remaining NaN values\n",
    "else:\n",
    "    print(\"Feature matrix 'X' is not defined. Please check previous steps.\")\n",
    "\n",
    "if 'X' in locals():\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Target vector shape: {y.shape}\")\n",
    "    print(f\"Number of classes: {y.nunique()}\")\n",
    "else:\n",
    "    print(\"Feature matrix 'X' is not defined. Please check previous steps.\")\n",
    "\n",
    "# Check class distribution before splitting\n",
    "if 'y' in locals():\n",
    "    class_distribution = y.value_counts().sort_index()\n",
    "    print(f\"\\nOriginal class distribution:\")\n",
    "    for class_idx, count in class_distribution.items():\n",
    "        class_name = reverse_target_mapping[class_idx]\n",
    "        percentage = count / len(y) * 100\n",
    "        print(f\"Class {class_idx} ({class_name}): {count:,} samples ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"Target vector 'y' is not defined. Please check previous steps.\")\n",
    "\n",
    "# First split: Train/Test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Second split: Train/Validation (64/16 of original, maintaining 80/20 split)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,  # 0.2 of 0.8 = 0.16 of total\n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"\\nData splitting completed:\")\n",
    "print(f\"Training set: {X_train_final.shape[0]:,} samples ({X_train_final.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(f\"\\nClass distribution verification:\")\n",
    "sets = {'Train': y_train_final, 'Validation': y_val, 'Test': y_test}\n",
    "\n",
    "for set_name, y_set in sets.items():\n",
    "    print(f\"      {set_name} set:\")\n",
    "    set_distribution = y_set.value_counts().sort_index()\n",
    "    for class_idx, count in set_distribution.items():\n",
    "        class_name = reverse_target_mapping[class_idx]\n",
    "        percentage = count / len(y_set) * 100\n",
    "        print(f\"         Class {class_idx} ({class_name}): {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Feature statistics\n",
    "print(f\"\\nFeature Statistics:\")\n",
    "print(f\"      ‚Ä¢ Total features: {X.shape[1]}\")\n",
    "print(f\"      ‚Ä¢ Numerical features: {len([col for col in feature_columns if any(x in col for x in ['age', 'lesion_count', 'diagnosis_confidence'])])}\")\n",
    "print(f\"      ‚Ä¢ Categorical features: {len([col for col in feature_columns if not any(x in col for x in ['age', 'lesion_count', 'diagnosis_confidence'])])}\")\n",
    "print(f\"      ‚Ä¢ Feature value range: [{X.min().min():.3f}, {X.max().max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf7b46",
   "metadata": {},
   "source": [
    "## Step 5: Class Imbalance Handling & Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db4a4e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Class Imbalance Handling:\n",
      "   üìä Imbalance Analysis:\n",
      "      ‚Ä¢ Majority class: 4,291 samples\n",
      "      ‚Ä¢ Minority class: 74 samples\n",
      "      ‚Ä¢ Imbalance ratio: 58.0:1\n",
      "\\n   üîÑ Applying SMOTE for class balancing...\n",
      "      ‚úÖ SMOTE completed:\n",
      "         Original training samples: 6,409\n",
      "         SMOTE training samples: 30,037\n",
      "      üìä SMOTE class distribution:\n",
      "         Class 0 (bkl): 4,291 (14.3%)\n",
      "         Class 1 (nv): 4,291 (14.3%)\n",
      "         Class 2 (df): 4,291 (14.3%)\n",
      "         Class 3 (mel): 4,291 (14.3%)\n",
      "         Class 4 (vasc): 4,291 (14.3%)\n",
      "         Class 5 (bcc): 4,291 (14.3%)\n",
      "         Class 6 (akiec): 4,291 (14.3%)\n",
      "\\nüéØ Feature Selection:\n",
      "   üìä Mutual Information Feature Selection:\n",
      "      ‚Ä¢ Selected features: 15\n",
      "      ‚Ä¢ Top 5 features by MI score:\n",
      "         1. is_high_risk: 0.4427\n",
      "         2. dx_type_encoded: 0.2621\n",
      "         3. diagnosis_confidence_scaled: 0.2606\n",
      "         4. diagnosis_confidence: 0.2579\n",
      "         5. diagnosis_confidence_minmax: 0.2456\n",
      "   üìä Mutual Information Feature Selection:\n",
      "      ‚Ä¢ Selected features: 15\n",
      "      ‚Ä¢ Top 5 features by MI score:\n",
      "         1. is_high_risk: 0.4427\n",
      "         2. dx_type_encoded: 0.2621\n",
      "         3. diagnosis_confidence_scaled: 0.2606\n",
      "         4. diagnosis_confidence: 0.2579\n",
      "         5. diagnosis_confidence_minmax: 0.2456\n",
      "\\n   üå≥ Random Forest Feature Importance:\n",
      "      ‚Ä¢ Top 10 features by importance:\n",
      "         1. is_high_risk: 0.2928\n",
      "         2. diagnosis_confidence_scaled: 0.0538\n",
      "         3. diagnosis_confidence_minmax: 0.0398\n",
      "         4. dx_type_encoded: 0.0344\n",
      "         5. lesion_count: 0.0330\n",
      "         6. age_scaled: 0.0329\n",
      "         7. age_normalized: 0.0328\n",
      "         8. diagnosis_confidence: 0.0325\n",
      "         9. lesion_count_scaled: 0.0322\n",
      "         10. lesion_count_minmax: 0.0314\n",
      "\\n   ‚úÖ Final feature selection:\n",
      "      ‚Ä¢ Selected 20 most important features\n",
      "      ‚Ä¢ Feature reduction: 51 ‚Üí 20\n",
      "\\nüìä Final Dataset Shapes:\n",
      "   ‚Ä¢ Training (original): (6409, 20)\n",
      "   ‚Ä¢ Training (SMOTE): (30037, 20)\n",
      "   ‚Ä¢ Validation: (1603, 20)\n",
      "   ‚Ä¢ Test: (2003, 20)\n",
      "\\n   üå≥ Random Forest Feature Importance:\n",
      "      ‚Ä¢ Top 10 features by importance:\n",
      "         1. is_high_risk: 0.2928\n",
      "         2. diagnosis_confidence_scaled: 0.0538\n",
      "         3. diagnosis_confidence_minmax: 0.0398\n",
      "         4. dx_type_encoded: 0.0344\n",
      "         5. lesion_count: 0.0330\n",
      "         6. age_scaled: 0.0329\n",
      "         7. age_normalized: 0.0328\n",
      "         8. diagnosis_confidence: 0.0325\n",
      "         9. lesion_count_scaled: 0.0322\n",
      "         10. lesion_count_minmax: 0.0314\n",
      "\\n   ‚úÖ Final feature selection:\n",
      "      ‚Ä¢ Selected 20 most important features\n",
      "      ‚Ä¢ Feature reduction: 51 ‚Üí 20\n",
      "\\nüìä Final Dataset Shapes:\n",
      "   ‚Ä¢ Training (original): (6409, 20)\n",
      "   ‚Ä¢ Training (SMOTE): (30037, 20)\n",
      "   ‚Ä¢ Validation: (1603, 20)\n",
      "   ‚Ä¢ Test: (2003, 20)\n"
     ]
    }
   ],
   "source": [
    "# Class Imbalance Handling\n",
    "print(\"‚öñÔ∏è Class Imbalance Handling:\")\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "class_counts = y_train_final.value_counts().sort_index()\n",
    "majority_count = class_counts.max()\n",
    "minority_count = class_counts.min()\n",
    "imbalance_ratio = majority_count / minority_count\n",
    "\n",
    "print(f\"   üìä Imbalance Analysis:\")\n",
    "print(f\"      ‚Ä¢ Majority class: {majority_count:,} samples\")\n",
    "print(f\"      ‚Ä¢ Minority class: {minority_count:,} samples\")\n",
    "print(f\"      ‚Ä¢ Imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "\n",
    "# Apply SMOTE for oversampling\n",
    "if imbalance_ratio > 2:  # Apply balancing if significantly imbalanced\n",
    "    print(f\"\\\\n   üîÑ Applying SMOTE for class balancing...\")\n",
    "    \n",
    "    # SMOTE\n",
    "    smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train_final, y_train_final)\n",
    "    \n",
    "    print(f\"      ‚úÖ SMOTE completed:\")\n",
    "    print(f\"         Original training samples: {len(X_train_final):,}\")\n",
    "    print(f\"         SMOTE training samples: {len(X_train_smote):,}\")\n",
    "    \n",
    "    # Verify SMOTE distribution\n",
    "    smote_distribution = pd.Series(y_train_smote).value_counts().sort_index()\n",
    "    print(f\"      üìä SMOTE class distribution:\")\n",
    "    for class_idx, count in smote_distribution.items():\n",
    "        class_name = reverse_target_mapping[class_idx]\n",
    "        percentage = count / len(y_train_smote) * 100\n",
    "        print(f\"         Class {class_idx} ({class_name}): {count:,} ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Dataset is relatively balanced, no SMOTE applied\")\n",
    "    X_train_smote, y_train_smote = X_train_final.copy(), y_train_final.copy()\n",
    "\n",
    "# Feature Selection\n",
    "print(f\"\\\\nüéØ Feature Selection:\")\n",
    "\n",
    "# 1. Mutual Information\n",
    "mi_selector = SelectKBest(score_func=mutual_info_classif, k=15)\n",
    "X_train_mi = mi_selector.fit_transform(X_train_final, y_train_final)\n",
    "\n",
    "# Get selected features\n",
    "selected_features_mi = np.array(modeling_features)[mi_selector.get_support()]\n",
    "mi_scores = mi_selector.scores_[mi_selector.get_support()]\n",
    "\n",
    "print(f\"   üìä Mutual Information Feature Selection:\")\n",
    "print(f\"      ‚Ä¢ Selected features: {len(selected_features_mi)}\")\n",
    "print(f\"      ‚Ä¢ Top 5 features by MI score:\")\n",
    "mi_ranking = sorted(zip(selected_features_mi, mi_scores), key=lambda x: x[1], reverse=True)\n",
    "for i, (feature, score) in enumerate(mi_ranking[:5]):\n",
    "    print(f\"         {i+1}. {feature}: {score:.4f}\")\n",
    "\n",
    "# 2. Random Forest Feature Importance\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_selector.fit(X_train_final, y_train_final)\n",
    "\n",
    "feature_importance = rf_selector.feature_importances_\n",
    "importance_ranking = sorted(zip(modeling_features, feature_importance), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\\\n   üå≥ Random Forest Feature Importance:\")\n",
    "print(f\"      ‚Ä¢ Top 10 features by importance:\")\n",
    "for i, (feature, importance) in enumerate(importance_ranking[:10]):\n",
    "    print(f\"         {i+1}. {feature}: {importance:.4f}\")\n",
    "\n",
    "# Select top features for final model\n",
    "top_k_features = 20\n",
    "selected_features_final = [feat for feat, _ in importance_ranking[:top_k_features]]\n",
    "\n",
    "print(f\"\\\\n   ‚úÖ Final feature selection:\")\n",
    "print(f\"      ‚Ä¢ Selected {len(selected_features_final)} most important features\")\n",
    "print(f\"      ‚Ä¢ Feature reduction: {len(feature_columns)} ‚Üí {len(selected_features_final)}\")\n",
    "\n",
    "# Create final feature sets\n",
    "X_train_final_selected = X_train_final[selected_features_final]\n",
    "X_val_final = X_val[selected_features_final]\n",
    "X_test_final = X_test[selected_features_final]\n",
    "X_train_smote_selected = X_train_smote[selected_features_final] if 'X_train_smote' in locals() else X_train_final_selected\n",
    "\n",
    "print(f\"\\\\nüìä Final Dataset Shapes:\")\n",
    "print(f\"   ‚Ä¢ Training (original): {X_train_final_selected.shape}\")\n",
    "print(f\"   ‚Ä¢ Training (SMOTE): {X_train_smote_selected.shape}\")\n",
    "print(f\"   ‚Ä¢ Validation: {X_val_final.shape}\")\n",
    "print(f\"   ‚Ä¢ Test: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c195a1",
   "metadata": {},
   "source": [
    "## Step 6: Save Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe92ee12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving Processed Datasets:\n",
      "   ‚úÖ Saved X_train: ../Dataset/processed\\X_train.csv\n",
      "   ‚úÖ Saved X_train: ../Dataset/processed\\X_train.csv\n",
      "   ‚úÖ Saved X_train_smote: ../Dataset/processed\\X_train_smote.csv\n",
      "   ‚úÖ Saved y_train: ../Dataset/processed\\y_train.csv\n",
      "   ‚úÖ Saved y_train_smote: ../Dataset/processed\\y_train_smote.csv\n",
      "   ‚úÖ Saved X_val: ../Dataset/processed\\X_val.csv\n",
      "   ‚úÖ Saved y_val: ../Dataset/processed\\y_val.csv\n",
      "   ‚úÖ Saved X_test: ../Dataset/processed\\X_test.csv\n",
      "   ‚úÖ Saved y_test: ../Dataset/processed\\y_test.csv\n",
      "   ‚úÖ Saved metadata: ../Dataset/processed\\preprocessing_metadata.pkl\n",
      "   ‚úÖ Saved feature list: ../Dataset/processed\\selected_features.txt\n",
      "   ‚úÖ Saved summary report: ../Dataset/processed\\preprocessing_summary.txt\n",
      "\\nüéâ PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "\\nüìÅ Processed files saved to: ../Dataset/processed\n",
      "   üìä Ready for model training in the Models/ folder\n",
      "   üöÄ Use these datasets to train your ML models\n",
      "   ‚úÖ Saved X_train_smote: ../Dataset/processed\\X_train_smote.csv\n",
      "   ‚úÖ Saved y_train: ../Dataset/processed\\y_train.csv\n",
      "   ‚úÖ Saved y_train_smote: ../Dataset/processed\\y_train_smote.csv\n",
      "   ‚úÖ Saved X_val: ../Dataset/processed\\X_val.csv\n",
      "   ‚úÖ Saved y_val: ../Dataset/processed\\y_val.csv\n",
      "   ‚úÖ Saved X_test: ../Dataset/processed\\X_test.csv\n",
      "   ‚úÖ Saved y_test: ../Dataset/processed\\y_test.csv\n",
      "   ‚úÖ Saved metadata: ../Dataset/processed\\preprocessing_metadata.pkl\n",
      "   ‚úÖ Saved feature list: ../Dataset/processed\\selected_features.txt\n",
      "   ‚úÖ Saved summary report: ../Dataset/processed\\preprocessing_summary.txt\n",
      "\\nüéâ PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "\\nüìÅ Processed files saved to: ../Dataset/processed\n",
      "   üìä Ready for model training in the Models/ folder\n",
      "   üöÄ Use these datasets to train your ML models\n"
     ]
    }
   ],
   "source": [
    "# Save Processed Datasets\n",
    "print(\"üíæ Saving Processed Datasets:\")\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Create processed data directory\n",
    "processed_dir = '../Dataset/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Save train/validation/test splits\n",
    "datasets = {\n",
    "    'X_train': X_train_final_selected,\n",
    "    'X_train_smote': X_train_smote_selected,\n",
    "    'y_train': y_train_final,\n",
    "    'y_train_smote': y_train_smote,\n",
    "    'X_val': X_val_final,\n",
    "    'y_val': y_val,\n",
    "    'X_test': X_test_final,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "# Save as CSV files\n",
    "for name, data in datasets.items():\n",
    "    filepath = os.path.join(processed_dir, f'{name}.csv')\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data.to_csv(filepath, index=False)\n",
    "    else:  # Series (target variables)\n",
    "        pd.DataFrame(data).to_csv(filepath, index=False)\n",
    "    print(f\"   ‚úÖ Saved {name}: {filepath}\")\n",
    "\n",
    "# Save metadata and mappings\n",
    "metadata = {\n",
    "    'feature_columns': selected_features_final,\n",
    "    'target_mapping': target_mapping,\n",
    "    'reverse_target_mapping': reverse_target_mapping,\n",
    "    'label_encoders': label_encoders,\n",
    "    'scalers': scalers,\n",
    "    'feature_importance_ranking': importance_ranking[:20],\n",
    "    'dataset_shapes': {\n",
    "        'original': df.shape,\n",
    "        'engineered': df_fe.shape,\n",
    "        'final_features': len(selected_features_final),\n",
    "        'train': X_train_final_selected.shape,\n",
    "        'train_smote': X_train_smote_selected.shape,\n",
    "        'validation': X_val_final.shape,\n",
    "        'test': X_test_final.shape\n",
    "    },\n",
    "    'class_distribution': {\n",
    "        'original': dict(y.value_counts().sort_index()),\n",
    "        'train': dict(y_train_final.value_counts().sort_index()),\n",
    "        'train_smote': dict(pd.Series(y_train_smote).value_counts().sort_index()),\n",
    "        'validation': dict(y_val.value_counts().sort_index()),\n",
    "        'test': dict(y_test.value_counts().sort_index())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata as pickle\n",
    "metadata_path = os.path.join(processed_dir, 'preprocessing_metadata.pkl')\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"   ‚úÖ Saved metadata: {metadata_path}\")\n",
    "\n",
    "# Save feature list as text file\n",
    "feature_list_path = os.path.join(processed_dir, 'selected_features.txt')\n",
    "with open(feature_list_path, 'w') as f:\n",
    "    f.write(\"Selected Features for Model Training:\\\\n\")\n",
    "    f.write(\"=\"*50 + \"\\\\n\")\n",
    "    for i, feature in enumerate(selected_features_final, 1):\n",
    "        f.write(f\"{i:2d}. {feature}\\\\n\")\n",
    "print(f\"   ‚úÖ Saved feature list: {feature_list_path}\")\n",
    "\n",
    "# Create preprocessing summary report\n",
    "summary_path = os.path.join(processed_dir, 'preprocessing_summary.txt')\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"PREPROCESSING SUMMARY REPORT\\\\n\")\n",
    "    f.write(\"=\"*60 + \"\\\\n\\\\n\")\n",
    "    \n",
    "    f.write(\"üìä DATASET OVERVIEW:\\\\n\")\n",
    "    f.write(f\"   ‚Ä¢ Original samples: {df.shape[0]:,}\\\\n\")\n",
    "    f.write(f\"   ‚Ä¢ Original features: {df.shape[1]}\\\\n\")\n",
    "    f.write(f\"   ‚Ä¢ Final features: {len(selected_features_final)}\\\\n\")\n",
    "    f.write(f\"   ‚Ä¢ Feature reduction: {(1 - len(selected_features_final)/df.shape[1])*100:.1f}%\\\\n\\\\n\")\n",
    "    \n",
    "    f.write(\"üîß FEATURE ENGINEERING:\\\\n\")\n",
    "    f.write(f\"   ‚Ä¢ Age groups created\\\\n\")\n",
    "    f.write(f\"   ‚Ä¢ Binary indicators added\\\\n\")\n",
    "    f.write(f\"   ‚Ä¢ Body region grouping\\\\n\")\n",
    "    f.write(f\"   ‚Ä¢ Risk categorization\\\\n\\\\n\")\n",
    "    \n",
    "    f.write(\"‚öñÔ∏è CLASS BALANCING:\\\\n\")\n",
    "    f.write(f\"   ‚Ä¢ Imbalance ratio: {imbalance_ratio:.1f}:1\\\\n\")\n",
    "    f.write(f\"   ‚Ä¢ SMOTE applied: {'Yes' if imbalance_ratio > 2 else 'No'}\\\\n\")\n",
    "    f.write(f\"   ‚Ä¢ Final training samples: {len(X_train_smote_selected):,}\\\\n\\\\n\")\n",
    "    \n",
    "    f.write(\"‚úÇÔ∏è DATA SPLITTING:\\\\n\")\n",
    "    f.write(f\"   ‚Ä¢ Training: {X_train_final_selected.shape[0]:,} ({X_train_final_selected.shape[0]/len(X)*100:.1f}%)\\\\n\")\n",
    "    f.write(f\"   ‚Ä¢ Validation: {X_val_final.shape[0]:,} ({X_val_final.shape[0]/len(X)*100:.1f}%)\\\\n\")\n",
    "    f.write(f\"   ‚Ä¢ Test: {X_test_final.shape[0]:,} ({X_test_final.shape[0]/len(X)*100:.1f}%)\\\\n\\\\n\")\n",
    "    \n",
    "    f.write(\"üéØ TOP FEATURES:\\\\n\")\n",
    "    for i, (feature, importance) in enumerate(importance_ranking[:10], 1):\n",
    "        f.write(f\"   {i:2d}. {feature}: {importance:.4f}\\\\n\")\n",
    "\n",
    "print(f\"   ‚úÖ Saved summary report: {summary_path}\")\n",
    "\n",
    "print(f\"\\\\nüéâ PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"\\\\nüìÅ Processed files saved to: {processed_dir}\")\n",
    "print(f\"   üìä Ready for model training in the Models/ folder\")\n",
    "print(f\"   üöÄ Use these datasets to train your ML models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32706b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering & Data Cleaning notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
